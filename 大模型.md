
---

# 大模型基础概念与原理

## 1. Token 的含义与分词方式

### 1.1 Token 是什么

* Token 是模型处理文本的基本单位：可以是字、子串、符号或空白等“文字片段”。
* 训练前会先构建一个词表（token 片段 ↔ 整数 ID 的双向映射）。
* 推理/训练时，所有输入输出都会先被 tokenizer 转成 token 序列，再送入模型。

### 1.2 常见的分词粒度

* 词级（word-level）

  * 优点：序列更短，直觉更强。
  * 缺点：词表容易过大；新词、错别字、拼写变体易 OOV（Out-Of-Vocabulary），泛化差。
* 字符级（char-level）

  * 优点：几乎无 OOV，覆盖性强。
  * 缺点：序列显著变长，计算负担重；高层语义更难学习（词被拆得很碎）。
* 子词级（subword-level）

  * 现阶段主流方案：在“覆盖性”和“序列长度”之间折中。
  * 常见算法：**BPE（Byte Pair Encoding）**、**WordPiece（BERT）**、**Unigram（SentencePiece）**。
  * 核心思想：用统计方法学习“常见字符串片段”作为 token；不常见的词被拆成更常见的片段。
  * 示例：understand / understanding / understood 可能被拆成：

    * ["under", "stand", "ing"] 或保留部分整词 token（视词表学习结果而定）

### 1.3 中文是如何做 tokenization 的

* 传统中文 NLP 常用“中文分词工具”（如 jieba / THULAC / HanLP），常见思路大致三类：

  * 词典 + 规则：依赖大词库，配合最大匹配等规则切分。
  * 统计模型：HMM、CRF 等，根据上下文概率决定分词边界。
  * 深度学习模型：BiLSTM、Transformer 做序列标注（B/M/E/S 等标签）。
* 现代大模型通常不采用人工词典驱动的“语义分词”，而是：

  * 在混合语料上直接训练子词词表；
  * 多数情况下一个汉字≈一个 token；
  * 高频短词可能合并成一个 token；
  * 标点、空格、换行往往也会作为独立 token 处理。
* **重点差异**：

  * 传统分词目标：更接近“语义上的词”；
  * LLM 的 tokenization 目标：更偏“统计压缩与可学习性”，让训练更稳定、更高效。

### 1.4 子词分词：以 BPE 为例

* BPE（Byte Pair Encoding）的直观流程：

  1. 将文本按最小单位切开（如字符/字节）
  2. 统计相邻 token 对出现的频次
  3. 找到频次最高的一对并合并为新 token
  4. 重复 2)~3)，直到达到预设词表大小
  5. 推理阶段：从左到右，尽量匹配“可合并的最长片段”，直到无法再合并

* GPT 常见的 Byte-level BPE（字节级 BPE）

  * 先把文本编码成 UTF-8 字节序列，保证任意符号都可表示；
  * 再在字节序列上做 BPE 学习与合并；
  * 经验直觉：中文、英文、符号在 token 粒度上差异较大，但整体上通常可认为“短片段为主，高频片段会合并”。

---

## 2. LLM 的类型与分类（按基础结构）

本节从 Transformer 架构差异出发，系统说明三类基础结构：Encoder-only、Decoder-only、Encoder-Decoder。核心区别不在“有没有 Transformer”，而在三点：

* 注意力可见性（mask 规则）
* 是否存在 cross-attention
* 预训练目标对应的概率分解方式

应用层的能力（指令微调、对齐、RAG、工具调用、Agent 等）通常建立在这些基础结构之上。

### 2.1 共同底座：Transformer 的基本计算

三类结构共享组件：token embedding、位置编码（绝对/相对）、多头自注意力、前馈网络（FFN）、残差连接与层归一化。

注意力基本形式：

$$
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

因此，“结构分类”的本质是在同一注意力算子之上，通过不同的 mask 与模块连接方式塑造信息流约束与能力边界。

---

### 2.2 Encoder-only（仅编码器：理解 / 表征）

* 结构：多层 Encoder block 堆叠；使用双向 self-attention。
* mask 特性：主要是 padding mask，不使用因果遮挡。
* 输出：上下文融合后的隐藏表示

$$
H=\mathrm{Encoder}(X)
$$

* 典型预训练目标：MLM（Masked Language Modeling）

  * 随机遮住一部分 token，在左右上下文可见条件下预测被遮住内容：

$$
\max \sum_{t\in \mathcal{M}} \log p(x_t \mid x_{\setminus \mathcal{M}})
$$

* 能力侧重：理解、分类、匹配、检索、重排序、Embedding 构建等。
* 不擅长点：开放式长文本生成不是其强项。

---

### 2.3 Decoder-only（仅解码器：自回归生成）

* 结构：多层 Decoder block 堆叠；核心是因果（causal）self-attention。
* mask 特性：严格的下三角可见性，禁止“看未来 token”：

$$
\mathrm{mask}_{i,j}=
\begin{cases}
1, & j \le i \
0, & j > i
\end{cases}
$$

* 典型预训练目标：CLM（Causal Language Modeling），即下一词预测：

$$
p(x_{1:T})=\prod_{t=1}^{T} p(x_t \mid x_{1:t-1})
$$

* 推理：按 token 逐步生成。
* 工程优化：常用 KV cache 缓存历史 $K,V$，新 token 只需计算当前 $Q$ 与缓存交互，以降低推理开销。
* 能力侧重：通用对话、开放式生成、代码生成与 Agent 基座生态。

---

### 2.4 Encoder-Decoder（编码器-解码器：Seq2Seq）

* 结构：编码器负责对输入双向理解；解码器在因果约束下生成输出。
* 连接：通过 cross-attention 让输出强条件依赖输入。

$$
H=\mathrm{Encoder}(X)
$$

$$
Y=\mathrm{Decoder}(Y_{1:t-1}, H)
$$

cross-attention 可视为解码器用自身查询选择性读取编码器输出：

$$
\mathrm{Attention}(Q_{dec}, K_{enc}, V_{enc})
$$

* 预训练目标：常见为去噪自编码或统一 text-to-text（如“输入→输出”的统一范式）。
* 能力侧重：翻译、摘要、信息抽取式生成、指令到结构化文本等“强输入约束生成”。
* 代价：结构更重、计算链更复杂；在开放域超大规模对话生态中相对 Decoder-only 不那么普及。

---

### 2.5 三类结构核心对照

| 结构类型            | 主要组成              | Self-Attention 可看未来？   | Cross-Attention | 典型预训练目标      | 更擅长的任务              |
| --------------- | ----------------- | ---------------------- | --------------- | ------------ | ------------------- |
| Encoder-only    | Encoder 堆叠        | 可以（双向）                 | 无               | MLM          | 理解、分类、检索、Embedding  |
| Decoder-only    | Decoder 堆叠        | 不可（因果）                 | 无               | CLM          | 自由生成、对话、代码、Agent 基座 |
| Encoder-Decoder | Encoder + Decoder | Encoder 可 / Decoder 不可 | 有               | Seq2Seq / 去噪 | 翻译、摘要、强条件生成         |

---

### 2.6 结构选择的简洁结论

* 通用对话 / 代码生成 / 工具与 Agent 编排：优先 **Decoder-only**。
* 语义表示 / 匹配 / 检索 / 重排序：优先 **Encoder-only**。
* 强输入约束输出（翻译/摘要/抽取式生成/结构化生成）：优先 **Encoder-Decoder**。

---


## 3. 语言模型基础：自回归、n-gram 与信息论

本节从概率论视角定义语言模型，对比古典统计方法（n-gram）与现代神经网络方法（Neural AR）的本质区别，并引入核心评估指标。

---

### 3.1 语言模型的数学定义（自回归通用形式）

无论模型架构如何（n-gram、RNN、Transformer），语言模型的核心任务始终是计算一个词序列的**联合概率**。

* **联合概率分解（链式法则）**：将长序列生成问题转化为一系列条件概率的乘积：

```math
p(x_{1:T})=\prod_{t=1}^{T} p(x_t \mid x_{1:t-1})


* **自回归（Autoregressive, AR）特性**

  * **方向性**：严格从左向右（Left-to-Right）。
  * **依赖性**：时刻 t 的预测依赖于 t 之前的历史 x_{1:t-1}。
  * **训练目标**：最大化似然估计（MLE），即最大化真实文本数据的概率。

---

### 3.2 n-gram 模型（古典统计逼近）

在深度学习之前，由于直接建模 p(x_t | x_{1:t-1}) 的参数空间随序列长度呈指数级爆炸，必须引入**马尔可夫假设（Markov Assumption）**进行简化。

* **核心假设**：当前词的出现仅依赖于它前面 n-1 个词，而非整个历史。

```math
p(x_t \mid x_{1:t-1}) \approx p(x_t \mid x_{t-n+1:t-1})
```

* **参数估计（基于计数）**：通过统计语料库中的词频来估算概率（以 Bigram 为例）：

```math
p(w_t \mid w_{t-1})=\frac{\mathrm{Count}(w_{t-1}, w_t)}{\mathrm{Count}(w_{t-1})}
```

* **面试高频考点：n-gram 的致命缺陷**

  1. **数据稀疏性（Sparsity）**：如果某个 n-gram 在训练集中没出现，概率为 0，导致整个句子概率为 0。
     *解决方案*：平滑技术（Smoothing），如 Laplace 平滑（加1法）、Kneser–Ney 平滑。
  2. **长距离依赖缺失**：无法捕捉超过窗口 n 的语义关联（如主谓距离很远时）。
  3. **参数爆炸**：模型体积随 n 指数增长（|V|^n），导致 n 无法设得很大（通常 n ≤ 5）。

---

### 3.3 信息理论与评估指标

如何衡量语言模型的好坏？不能只看准确率（Accuracy），因为语言是多样的。我们需要衡量概率分布的质量。

* **熵（Entropy）**：衡量随机变量的不确定性。文本及其分布越“混乱/不可预测”，熵越高。

```math
H(P)=-\sum_x P(x)\log P(x)
```

* **交叉熵（Cross-Entropy）——训练时的 Loss**：衡量“真实分布 P”与“模型预测分布 Q”之间的差异。

```math
H(P,Q)=-\sum_x P(x)\log Q(x)
```

在语言模型中，最小化交叉熵等价于最大化对数似然。

* **困惑度（Perplexity, PPL）——评估时的 Metric**：交叉熵的指数形式，物理含义更直观。

```math
\mathrm{PPL}=\exp(H(P,Q))
```

* **直观解释（分支系数）**：PPL = k 意味着模型在预测下一个词时，困惑程度相当于在 k 个等概率的词中盲猜。
* **判定**：PPL 越低，模型越好（越确定）。

---

### 3.4 从 n-gram 到神经语言模型（Neural LM）

现代 LLM（如 GPT）依然是自回归模型，但用**神经网络**替代了**统计计数**。

* **核心进化点**

  1. **分布式表示（Word Embeddings）**：将离散的词映射为稠密向量。
     *解决稀疏性*：即使没见过“吃 苹果”，如果见过“吃 梨”且“苹果”与“梨”向量接近，模型也能泛化。
  2. **建模函数（Function Approximation）**：不再查表，而是通过网络参数 θ 计算概率分布：

```math
p(x_t \mid x_{<t})=\mathrm{softmax}\left(f_\theta(x_{<t})\right)
```

3. **超长上下文建模**

   * RNN/LSTM：理论上无限，实际上受限于梯度消失/爆炸。
   * Transformer：受限于 context window，但能通过 Attention 机制直接捕获长距离依赖。

---

### 3.5 核心对照表：古典 vs 现代

| 特性        | n-gram（统计语言模型）       | Neural LM（Transformer/RNN）    |     |              |   |       |
| --------- | -------------------- | ----------------------------- | --- | ------------ | - | ----- |
| **上下文长度** | 固定且极短（n）             | 长窗口（Context Window，如 4k–128k） |     |              |   |       |
| **参数空间**  | 随 n 指数爆炸（            | V                             | ^n） | 随网络深度/宽度增长，与 | V | ^n 无关 |
| **泛化能力**  | 差（没见过的 n-gram 概率为 0） | 强（基于向量相似性泛化）                  |     |              |   |       |
| **计算本质**  | 查表、计数、除法             | 矩阵乘法、非线性激活、softmax            |     |              |   |       |
| **主要应用**  | 搜索建议、输入法（旧版）         | 生成式 AI、对话系统、翻译、代码             |     |              |   |       |

---

### 3.6 深度解析：统计计数（n-gram）vs 神经网络（NN）的本质差异

核心结论：**n-gram 是基于记忆的查表，神经网络是基于特征的函数拟合。**
根本差异可概括为：**离散符号的硬匹配（Hard Matching）** vs **连续向量的软计算（Soft Computation / Approximation）**。

#### 3.6.1 表征方式：离散 vs 分布式（核心差异）

* **统计计数（n-gram）—— 离散 / One-hot**

  * **世界观**：词是孤立的符号（ID）。
  * **致命伤**：词与词之间正交，无法表达语义相似性（猫-狗 与 猫-汽车 的相似度都为 0）。
  * **后果**：训练集只有“猫在跑”，测试遇到“狗在跑”，n-gram 会认为是陌生组合。

* **神经网络（NN）—— 分布式表示 / Embeddings**

  * **世界观**：词是语义空间中的点（向量）。
  * **优势**：相似词向量距离近，可通过点积/余弦相似度表达“语义邻近”。
  * **后果**：学会“猫在跑”后，因“狗 ≈ 猫”，可泛化到“狗在跑”——这就是泛化（Generalization）。

#### 3.6.2 计算本质：查表 vs 函数拟合

* **n-gram：Table Lookup（查表）**

  * 模型等价于一个巨大哈希表：Key = 上下文，Value = 概率。
  * Key 不存在 → 概率 0（需要平滑补丁）。

* **NN：Function Approximation（函数拟合）**

  * 模型是复杂函数 f_θ（矩阵乘法 + 激活 + softmax）。
  * 即便组合未出现，也能基于向量特征给出非零概率并输出合理分布。

#### 3.6.3 维度灾难：指数爆炸 vs 参数共享

* **n-gram 参数爆炸**

  * 词表大小 |V|，n-gram 组合规模约 |V|^n，参数随 n 指数增长，n 难以取大。
* **神经网络参数共享**

  * 参数量主要由隐藏维度与层数决定，与上下文长度 n 解耦。
  * 上下文变长更多增加计算量，但模型体积不随 n 指数爆炸。

#### 3.6.4 总结对照表（面试“背诵版”）

| 比较维度     | 统计语言模型（n-gram） | 神经语言模型（NNLM / Transformer） |     |                |
| -------- | -------------- | -------------------------- | --- | -------------- |
| **知识存储** | 显式记忆：直接记住共现计数  | 隐式知识：规律压缩进参数 θ             |     |                |
| **相似性**  | 符号正交：词与词无语义距离  | 向量相似：语义邻近可度量               |     |                |
| **未见数据** | 概率为 0（需平滑）     | 自动泛化（基于特征推断）               |     |                |
| **参数规模** | 随 n 指数爆炸（      | V                          | ^n） | 随网络规模增长，与 n 解耦 |
| **比喻**   | 图书管理员：查索引卡     | 侦探：根据规律推演                  |     |                |

#### 3.6.5 极简结论（一句话）

神经网络相比统计计数的根本区别在于：**它通过“分布式向量表示”解决了词与词之间的语义孤立问题，从而通过“连续函数拟合”缓解了 n-gram 的数据稀疏与维度灾难问题。**

