# 大模型基础概念与原理
## token的含义及分词方式
### token是什么
* 句子、段落、文档之类的文字片段，模型在训练前会先训练一个词表（token片段与整数ID之间的双向映射），之后所有的文本输入输出前都要先变成token序列，再喂给模型。
### 常见的分词粒度
* 词级（word-level）：词表过大，新词，错别字，拼写错误会变成OOV，很难处理。
* 字符级（chr-level）：解决了词级的问题，但是模型处理负担变重（“一个句子=多个token”），难以捕捉高层语义（词被拆的碎）
* 子词级（subword-level）：当前主流大模型采用的方式，词和字之间进行折中，常见算法**BPE（Byte Pair Encoding）**,**WordPiece(BERT用的）**,**Unigram(SetencePiece用的)**，**核心思想**是使用通统计学的方法，把常见的字符串片段当成token，把不常见的拆成常见的（understand, understanding, understood拆成["under", "stand", "ing", "understand", ...]）
### 中文是如何优化token的
* 传统中文NLP用中文分词工具（jieba、THULAC、HanLP），原理大致三类：1、词典+规则：使用一个大的词库，用最大匹配等规则进行切分。2、统计模型：用HMM、CRF等模型，根据上下文概率决定分词边界。3、深度学习模型：BiLSTM、Transformer做序列标注，给每个字打上B/M/E/S等标签。
* 现代大模型不会使用人工定义的中文字典进行分词，而是直接使用子词算法在混合语料上进行训练，大部分时候一个汉字就是一个token，高频的短词也可能作为一个token，标点符号和空格换行也是单独的token。
**重点差异**：传统分词想要得到“语义上的词”，“大模型tokenization”只关心“让模型好训练，好压缩”的统计单元。
### 子词级分词方式进一步理解
* BPE：Byte Pair Encoding的直观流程：
```md
1. 把文本按最小单位切开
2. 数相邻的对出现多少次
3. 找出现次数最多的一对
4. 重复 2、3 步骤，直到达到预设词表的大小
5. 推理阶段：从左到右，不断匹配最长的可合并片段，直到无法再合并
```
*GPT常用的Byte-level PBE：是字节级，现将文本转化成UTF-8，保证所有的符号都能表示，再进行BPE。一个汉字，英文词，符号，大概占0.75-1.5个token，中文用的比英文少。




<details>
  
<summary>点击展开查看代码</summary>
