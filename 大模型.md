
---

# 大模型基础概念与原理

## 1. Token 的含义与分词方式

### 1.1 Token 是什么

* Token 是模型处理文本的基本单位：可以是字、子串、符号或空白等“文字片段”。
* 训练前会先构建一个词表（token 片段 ↔ 整数 ID 的双向映射）。
* 推理/训练时，所有输入输出都会先被 tokenizer 转成 token 序列，再送入模型。

### 1.2 常见的分词粒度

* 词级（word-level）

  * 优点：序列更短，直觉更强。
  * 缺点：词表容易过大；新词、错别字、拼写变体易 OOV（Out-Of-Vocabulary），泛化差。
* 字符级（char-level）

  * 优点：几乎无 OOV，覆盖性强。
  * 缺点：序列显著变长，计算负担重；高层语义更难学习（词被拆得很碎）。
* 子词级（subword-level）

  * 现阶段主流方案：在“覆盖性”和“序列长度”之间折中。
  * 常见算法：**BPE（Byte Pair Encoding）**、**WordPiece（BERT）**、**Unigram（SentencePiece）**。
  * 核心思想：用统计方法学习“常见字符串片段”作为 token；不常见的词被拆成更常见的片段。
  * 示例：understand / understanding / understood 可能被拆成：

    * ["under", "stand", "ing"] 或保留部分整词 token（视词表学习结果而定）

### 1.3 中文是如何做 tokenization 的

* 传统中文 NLP 常用“中文分词工具”（如 jieba / THULAC / HanLP），常见思路大致三类：

  * 词典 + 规则：依赖大词库，配合最大匹配等规则切分。
  * 统计模型：HMM、CRF 等，根据上下文概率决定分词边界。
  * 深度学习模型：BiLSTM、Transformer 做序列标注（B/M/E/S 等标签）。
* 现代大模型通常不采用人工词典驱动的“语义分词”，而是：

  * 在混合语料上直接训练子词词表；
  * 多数情况下一个汉字≈一个 token；
  * 高频短词可能合并成一个 token；
  * 标点、空格、换行往往也会作为独立 token 处理。
* 重点差异：

  * 传统分词目标：更接近“语义上的词”；
  * LLM 的 tokenization 目标：更偏“统计压缩与可学习性”，让训练更稳定、更高效。

### 1.4 子词分词：以 BPE 为例

* BPE（Byte Pair Encoding）的直观流程：

  1. 将文本按最小单位切开（如字符/字节）
  2. 统计相邻 token 对出现的频次
  3. 找到频次最高的一对并合并为新 token
  4. 重复 2)~3)，直到达到预设词表大小
  5. 推理阶段：从左到右，尽量匹配“可合并的最长片段”，直到无法再合并

* GPT 常见的 Byte-level BPE（字节级 BPE）

  * 先把文本编码成 UTF-8 字节序列，保证任意符号都可表示；
  * 再在字节序列上做 BPE 学习与合并；
  * 经验直觉：中文、英文、符号在 token 粒度上差异较大，但整体上通常可认为“短片段为主，高频片段会合并”。

---

## 2. LLM 的类型与分类（按基础结构）

本节从 Transformer 架构差异出发，系统说明三类基础结构：Encoder-only、Decoder-only、Encoder-Decoder。核心区别不在“有没有 Transformer”，而在三点：

* 注意力可见性（mask 规则）
* 是否存在 cross-attention
* 预训练目标对应的概率分解方式

应用层的能力（指令微调、对齐、RAG、工具调用、Agent 等）通常建立在这些基础结构之上。

### 2.1 共同底座：Transformer 的基本计算

三类结构共享组件：token embedding、位置编码（绝对/相对）、多头自注意力、前馈网络（FFN）、残差连接与层归一化。

注意力基本形式：

[
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
]

因此，“结构分类”的本质是在同一注意力算子之上，通过不同的 mask 与模块连接方式塑造信息流约束与能力边界。

---

### 2.2 Encoder-only（仅编码器：理解 / 表征）

* 结构：多层 Encoder block 堆叠；使用双向 self-attention。
* mask 特性：主要是 padding mask，不使用因果遮挡。
* 输出：上下文融合后的隐藏表示

[
H=\mathrm{Encoder}(X)
]

* 典型预训练目标：MLM（Masked Language Modeling）

  * 随机遮住一部分 token，在左右上下文可见条件下预测被遮住内容：

[
\max \sum_{t\in \mathcal{M}} \log p(x_t \mid x_{\setminus \mathcal{M}})
]

* 能力侧重：理解、分类、匹配、检索、重排序、Embedding 构建等。
* 不擅长点：开放式长文本生成不是其强项。

---

### 2.3 Decoder-only（仅解码器：自回归生成）

* 结构：多层 Decoder block 堆叠；核心是因果（causal）self-attention。
* mask 特性：严格的下三角可见性，禁止“看未来 token”：

[
\mathrm{mask}_{i,j}=
\begin{cases}
1, & j \le i \
0, & j > i
\end{cases}
]

* 典型预训练目标：CLM（Causal Language Modeling），即下一词预测：

[
p(x_{1:T})=\prod_{t=1}^{T} p(x_t \mid x_{1:t-1})
]

* 推理：按 token 逐步生成。
* 工程优化：常用 KV cache 缓存历史 (K,V)，新 token 只需计算当前 (Q) 与缓存交互，以降低推理开销。
* 能力侧重：通用对话、开放式生成、代码生成与 Agent 基座生态。

---

### 2.4 Encoder-Decoder（编码器-解码器：Seq2Seq）

* 结构：编码器负责对输入双向理解；解码器在因果约束下生成输出。
* 连接：通过 cross-attention 让输出强条件依赖输入。

[
H=\mathrm{Encoder}(X)
]

[
Y=\mathrm{Decoder}(Y_{1:t-1}, H)
]

cross-attention 可视为解码器用自身查询选择性读取编码器输出：

[
\mathrm{Attention}(Q_{dec}, K_{enc}, V_{enc})
]

* 预训练目标：常见为去噪自编码或统一 text-to-text（如“输入→输出”的统一范式）。
* 能力侧重：翻译、摘要、信息抽取式生成、指令到结构化文本等“强输入约束生成”。
* 代价：结构更重、计算链更复杂；在开放域超大规模对话生态中相对 Decoder-only 不那么普及。

---

### 2.5 三类结构核心对照

| 结构类型            | 主要组成              | Self-Attention 可看未来？   | Cross-Attention | 典型预训练目标      | 更擅长的任务              |
| --------------- | ----------------- | ---------------------- | --------------- | ------------ | ------------------- |
| Encoder-only    | Encoder 堆叠        | 可以（双向）                 | 无               | MLM          | 理解、分类、检索、Embedding  |
| Decoder-only    | Decoder 堆叠        | 不可（因果）                 | 无               | CLM          | 自由生成、对话、代码、Agent 基座 |
| Encoder-Decoder | Encoder + Decoder | Encoder 可 / Decoder 不可 | 有               | Seq2Seq / 去噪 | 翻译、摘要、强条件生成         |

---

### 2.6 结构选择的简洁结论

* 通用对话 / 代码生成 / 工具与 Agent 编排：优先 **Decoder-only**。
* 语义表示 / 匹配 / 检索 / 重排序：优先 **Encoder-only**。
* 强输入约束输出（翻译/摘要/抽取式生成/结构化生成）：优先 **Encoder-Decoder**。
