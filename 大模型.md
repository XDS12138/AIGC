# 大模型基础概念与原理
## token的含义及分词方式
### token是什么
* 句子、段落、文档之类的文字片段，模型在训练前会先训练一个词表（token片段与整数ID之间的双向映射），之后所有的文本输入输出前都要先变成token序列，再喂给模型。
### 常见的分词粒度
* 词级（word-level）：词表过大，新词，错别字，拼写错误会变成OOV，很难处理。
* 字符级（chr-level）：解决了词级的问题，但是模型处理负担变重（“一个句子=多个token”），难以捕捉高层语义（词被拆的碎）
* 子词级（subword-level）：当前主流大模型采用的方式，词和字之间进行折中，常见算法**BPE（Byte Pair Encoding）**,**WordPiece(BERT用的）**,**Unigram(SetencePiece用的)**，**核心思想**是使用通统计学的方法，把常见的字符串片段当成token，把不常见的拆成常见的（understand, understanding, understood拆成["under", "stand", "ing", "understand", ...]）
### 中文是如何优化token的
* 传统中文NLP用中文分词工具（jieba、THULAC、HanLP），原理大致三类：1、词典+规则：使用一个大的词库，用最大匹配等规则进行切分。2、统计模型：用HMM、CRF等模型，根据上下文概率决定分词边界。3、深度学习模型：BiLSTM、Transformer做序列标注，给每个字打上B/M/E/S等标签。
* 现代大模型不会使用人工定义的中文字典进行分词，而是直接使用子词算法在混合语料上进行训练，大部分时候一个汉字就是一个token，高频的短词也可能作为一个token，标点符号和空格换行也是单独的token。
**重点差异**：传统分词想要得到“语义上的词”，“大模型tokenization”只关心“让模型好训练，好压缩”的统计单元。
### 子词级分词方式进一步理解
* BPE：Byte Pair Encoding的直观流程：
```md
1. 把文本按最小单位切开
2. 数相邻的对出现多少次
3. 找出现次数最多的一对
4. 重复 2、3 步骤，直到达到预设词表的大小
5. 推理阶段：从左到右，不断匹配最长的可合并片段，直到无法再合并
```
*GPT常用的Byte-level PBE：是字节级，现将文本转化成UTF-8，保证所有的符号都能表示，再进行BPE。一个汉字，英文词，符号，大概占0.75-1.5个token，中文用的比英文少。



---

# LLM 的类型和分类（按基础结构）

这一部分从 Transformer 架构差异出发，系统说明三类基础结构：Encoder-only、Decoder-only、Encoder-Decoder。核心区别不在“有没有 Transformer”，而在三点：注意力可见性（mask）、是否存在 cross-attention、预训练目标对应的概率分解方式。同一个模型在应用层可能同时具备指令微调、对齐、RAG、工具调用等能力，但这些都建立在基础结构之上。

---

## 0. 共同底座：Transformer 的基本计算

三类结构都共享以下组件：token embedding、位置编码（绝对/相对）、多头自注意力、前馈网络（FFN）、残差连接与层归一化。注意力基本形式为：

$$
\mathrm{Attention}(Q,K,V)=\mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d_k}}\right)V
$$

因此，“结构分类”的本质，是在同一套注意力算子基础上，用不同的 mask 规则与模块连接方式，来塑造模型的信息流约束与能力边界。

---

## 1. Encoder-only（仅编码器，偏理解/表征）

Encoder-only 由多层编码器块堆叠，每层使用双向 self-attention。双向的含义是：同一序列内任意 token 都可以相互注意（只需屏蔽 padding），不限制只能看过去。模型输出是一组上下文融合后的隐藏表示：

$$
H=\mathrm{Encoder}(X)
$$

其关键 mask 特性主要是 padding mask，而不使用因果遮挡。因此第 $i$ 个 token 的表示可融合全句信息，这对语义判别、上下文对齐和表示学习尤其有利。

典型预训练目标是 MLM（Masked Language Modeling）。训练时随机遮住一部分 token，让模型在左右上下文可见的条件下预测被遮住的内容：

$$
\max \sum_{t\in \mathcal{M}} \log p(x_t \mid x_{\setminus \mathcal{M}})
$$

能力上，Encoder-only 更像“读懂并编码”的系统，常用于分类、匹配、检索、重排序与 Embedding 构建等任务。它并不以开放式长文本生成见长。

---

## 2. Decoder-only（仅解码器，自回归生成）

Decoder-only 由多层解码器块堆叠，核心是因果（Causal）self-attention。每个 token 的表示严格基于历史上下文。

其关键差异来自因果 mask：注意力矩阵具有下三角可见性，保证模型不能“偷看未来 token”：

$$
\mathrm{mask}_{i,j}=
\begin{cases}
1, & j \le i \
0, & j > i
\end{cases}
$$

典型预训练目标是 CLM（Causal Language Modeling），即下一词预测。序列概率分解为：

$$
p(x_{1:T})=\prod_{t=1}^{T} p(x_t \mid x_{1:t-1})
$$

推理阶段按 token 逐步生成。为了提升长序列与多轮对话性能，工程上常使用 KV cache 缓存历史 $K,V$，使新 token 只需计算当前 $Q$ 并与缓存交互，从而显著降低推理开销。

能力上，Decoder-only 是现代通用对话与代码 LLM 的主流基础结构，目标统一、训练-推理一致、易于扩展指令微调与偏好对齐，并可进一步叠加 RAG 与工具调用提升可靠性。

---

## 3. Encoder-Decoder（编码器-解码器，Seq2Seq）

Encoder-Decoder 包含两个塔：编码器负责对输入做双向理解，解码器在因果约束下生成输出。二者通过 cross-attention 连接，形成“输入条件强约束的生成范式”：

$$
H=\mathrm{Encoder}(X)
$$

$$
Y=\mathrm{Decoder}(Y_{1:t-1}, H)
$$

cross-attention 可理解为解码器用自身查询 $Q_{dec}$ 去选择性读取编码器输出的 $K_{enc},V_{enc}$，从而让输出更严格地受输入控制：

$$
\mathrm{Attention}(Q_{dec}, K_{enc}, V_{enc})
$$

预训练目标常见于去噪自编码或统一的 text-to-text 框架。能力上，Encoder-Decoder 在翻译、摘要、信息抽取式生成以及指令到结构化描述等任务中表现稳定，但结构与计算链更重，在开放域超大规模对话生态中当前不如 Decoder-only 普及。

---

## 4. 三类结构的核心对照

| 结构类型            | 主要组成              | Self-Attention 可看未来？   | Cross-Attention | 典型预训练目标    | 更擅长的任务              |
| --------------- | ----------------- | ---------------------- | --------------- | ---------- | ------------------- |
| Encoder-only    | Encoder 堆叠        | 可以（双向）                 | 无               | MLM        | 理解、分类、检索、Embedding  |
| Decoder-only    | Decoder 堆叠        | 不可（因果）                 | 无               | CLM        | 自由生成、对话、代码、Agent 基座 |
| Encoder-Decoder | Encoder + Decoder | Encoder 可 / Decoder 不可 | 有               | Seq2Seq/去噪 | 翻译、摘要、强条件生成         |

---

## 5. 结构选择的简洁结论

如果目标是通用对话、代码生成和工具/智能体编排，Decoder-only 通常是最通用且生态最成熟的选择。若核心任务是语义表示、匹配、检索与重排序等判别式理解，Encoder-only 往往更高效稳定。若需要严格的“输入约束输出”，例如翻译、摘要或指令到结构化文本，Encoder-Decoder 通常更自然。

---

如果你还想，我可以把你这章里**BPE + 训练范式（预训练/指令微调/对齐）+ RAG/Agent** 也按同样风格与公式规范，给你一份“整章可直接替换版”。

